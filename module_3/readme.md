# Module 3 – GradCafe Data Analysis Web Application

## Overview

This project builds on data gathered from GradCafe and demonstrates relational database usage, SQL analytics, and dynamic webpage development using Flask. The application loads GradCafe applicant data into a PostgreSQL database, performs data analysis through SQL queries, and displays results on a stylized Flask web interface.

---

## Features Implemented

### Database Integration

* Loads cleaned GradCafe applicant data into PostgreSQL using `psycopg`
* Uses a structured relational schema for applicant data
* Prevents duplicate entries using a unique URL constraint

---

### SQL Data Analysis

The application performs the following analytical queries:

1. Number of applicants applying for Fall 2026
2. Percentage of international applicants
3. Average GPA, GRE Quantitative, GRE Verbal, and GRE Analytical Writing
4. Average GPA of American applicants for Fall 2026
5. Acceptance rate for Fall 2025
6. Average GPA of accepted applicants for Fall 2026
7. Count of applicants applying to Johns Hopkins University MS Computer Science
8. Count of accepted PhD Computer Science applicants to selected universities
9. Comparison of raw scraped data versus LLM-enhanced data
10. Custom analysis:

* Top 5 most common programs for Fall 2026
* Average GPA of international applicants

---

### Flask Web Application

* Uses Flask Blueprints for modular routing
* Uses HTML templates and CSS styling
* Displays SQL analytics dynamically
* Supports PostgreSQL database integration

---

### Pull Data Functionality (Part B)

The web interface includes:

#### Pull Data Button

* Executes Module 2 scraping logic
* Loads new GradCafe entries into PostgreSQL
* Uses subprocess execution
* Uses file-based locking to prevent concurrent scraping

#### Update Analysis Button

* Refreshes the webpage with the latest database results
* Disabled while data scraping is running

---

## Pull Data Concurrency Control

To prevent multiple simultaneous scraping operations, the application uses a file-based locking mechanism implemented in:

```
app/pages/pull_state.py
```

This module:

* Creates a lock file when Pull Data begins
* Prevents duplicate Pull Data execution
* Disables Pull Data and Update Analysis buttons while scraping runs
* Automatically removes the lock file when scraping completes

This ensures safe database updates and prevents concurrent scraper execution.

---

## Data Source and Database Population

For reliable database initialization and testing, the PostgreSQL database used in this project was populated using the instructor-provided dataset:

```
applicant_data.json (provided by Instructor Liv)
```

This ensured that:

* A complete and clean dataset was available for SQL analytics
* Module 3 functionality could be validated independently of scraping variability
* All required queries and webpage analytics operate correctly using verified data

The application also supports loading data generated by the Module 2 scraper when available.

---

## Module 2 Status and Structural Updates

Module 2 originally contained a GradCafe web scraper used to gather applicant data.

Several improvements were implemented to better align with assignment requirements, including:

* Migration from `urllib3` to `urllib`
* Additional field extraction support
* Improved error handling and parsing logic
* Enhanced data cleaning to standardize missing values
* Deduplication logic to prevent repeated entries
* Pagination protection to detect repeated survey pages

### Folder Structure Update

The original Module 2 submission contained an additional folder named:

```
WebScraper/
```

To improve clarity and simplify execution paths, the Module 2 codebase was reorganized so that:

* All scraping and cleaning scripts now reside directly under the `module_2` directory
* The extra `WebScraper` folder was removed
* Execution paths and imports were updated accordingly

---

## Module 2 Known Limitations

GradCafe’s server behavior occasionally returns repeated survey pages when accessed programmatically. This causes Module 2 scraping to sometimes stop early or fail to retrieve the full dataset reliably.

However:

* Module 2 scraping logic successfully retrieves data in limited scenarios
* Module 3 database loading, SQL analysis, and Flask functionality operate correctly
* Module 3 supports loading instructor-provided datasets as well as scraper-generated datasets

---

## Technologies Used

* Python 3.14
* Flask
* PostgreSQL
* psycopg
* HTML / CSS
* JSON
* urllib (standard library web requests)
* BeautifulSoup (Module 2 parsing only)

---

## Database Schema

The PostgreSQL database contains a single table named `applicants` with the following fields:

* Program
* Comments
* Date Added
* URL
* Status
* Term
* Citizenship Status
* GPA
* GRE Quantitative
* GRE Verbal
* GRE Analytical Writing
* Degree
* LLM Generated Program
* LLM Generated University

---

## Installation Instructions

### 1. Create Virtual Environment

```
python3 -m venv .venv
source .venv/bin/activate
```

---

### 2. Install Dependencies

```
pip install -r requirements.txt
```

---

### 3. Configure PostgreSQL Connection

Set environment variable:

```
export DATABASE_URL="postgresql://username:password@localhost:5432/gradcafe"
```

---

### 4. Load Applicant Data

```
python load_data.py --file applicant_data.json
```

---

### 5. Run SQL Queries

```
python query_data.py
```

---

### 6. Run Flask Application

```
python run.py
```

Open browser:

```
http://127.0.0.1:5000
```

---

## Known Limitations

* GradCafe pagination occasionally returns repeated pages, which may limit scraper dataset size
* Pull Data functionality depends on GradCafe server response consistency
* If the scraper is interrupted unexpectedly, the lock file may persist and must be manually removed
* Web scraping relies on publicly available user-submitted data, which may contain inaccuracies

---

## Academic Integrity

All code was implemented following assignment requirements using only permitted libraries and methodologies.

---

## Repository Structure

```
jhu_software_concepts/
├── module_2/
│   ├── scrape.py
│   ├── clean.py
│   ├── main.py
│   ├── applicant_data.json
│   ├── requirements.txt
│   └── README.md
│
├── module_3/
│   ├── app/
│   │   ├── pages/
│   │   │   ├── analysis.py
│   │   │   ├── db.py
│   │   │   └── pull_state.py
│   │   ├── templates/
│   │   │   ├── base.html
│   │   │   └── analysis.html
│   │   ├── static/
│   │   │   └── styles.css
│   │   └── __init__.py
│   ├── load_data.py
│   ├── query_data.py
│   ├── pull_data.py
│   ├── run.py
│   ├── requirements.txt
│   └── README.md
│
└── .gitignore
```

---

## Author Notes

This project demonstrates full-stack data pipeline development including scraping, cleaning, database integration, analytics, and web visualization.
